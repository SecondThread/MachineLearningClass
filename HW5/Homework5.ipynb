{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObz9QcUalQpAwC15ndUekb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SecondThread/MachineLearningClass/blob/master/HW5/Homework5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6wwXV4VX73",
        "colab_type": "text"
      },
      "source": [
        "#General Concepts\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H1m319Od0vb",
        "colab_type": "text"
      },
      "source": [
        "This class began with a discussion of an overview of deep learning, machine learning, and artificial intelligience. Initially they were described as distict, but closely related ideas, in which each was a subset of the next, in the order listed above. Formally, we can define them in the following ways:\n",
        "\n",
        "**Artificial Intellegence** - \"The science and engineering of making intelligent machines\"- John McCarthy. \n",
        "- One aspect of AI is symbolic AI, or Good Old-Fashioned AI, in which humans attempt to write a program to first classify human readable representations of the problem. Once the code has built this model of the problem, humans can write algorithms to make decisions from this point in an understandble and reasonable way, by abusing known properties of the problem. For instance, determining the winning move in a Rock/Paper/Scissors game would be a good goal for GOFAI. If a model can be accurately built to determine the move the opponent has made, casing out the best decision can give rise to reasonable output.\n",
        "- Symbolic AI can be thought of as taking in some set of input and rules, and returning whatever output fits that input and rules. In the Rock/Paper/Scissors example described above, it would take in the winning states for each move the opponent could make (rules), as well as the move the opponent made this turn (input). It would return the move to beat the opponent (output).\n",
        "\n",
        "**Machine Learning** - In contrast with symbolic AI, Machine Learning  is a type of AI in which a computer finds some way to make intelligent decisions without being explicitly told what to do in every situtation. On one hand, this makes it slightly more dangerous and unpredictable: without specifying explicit rules for what the machine will do, it isn't clear at the onset exactly what behavior we will get. On the other hand, this is often exactly what we want for certain difficult problems for which it is not easy or feasible for a human to write out a step-by-step set of instructions indicating what needs to be done in order for some result to be achieved.\n",
        "- To continue the Rock/Paper/Scissors analogy earlier, training a bot to play Rock/Paper/Scissors using machine learning would involve providing it with the input of the opponent, but not directly giving it the rules of the game. Then, after the bot makes a decision, you would tell it the result of the game, and allow it to update its model of what it should do in that situtation depending on whether it won or lost.\n",
        "\n",
        "**Deep Learning** - Deep learning is a further subset of machine learning that uses certain specific approaches in order to design a model to achieve some predefined goal. In general, Deep Nueral Networks are used in Deep Learning. Having several layers in this neural networks is ultimately what makes it 'deep'. The main vitue of having many layers is that it allows the network to identify nonlinear relationships between input variables, or features, mapping to different outputs. These nonlinear relationships are identified from the nonlinear activation function that is used within nuerons of the nueral network, or convolutional nueral network (as often used in image or sound processing for instance).\n",
        "\n",
        "**Supervised and Unsupervised Learning** - Although it isn't very relevant to the rest of this summary, The distinction between supervised and unsupervised learning is rather interesting, so I thought I would quickly mention it here. \n",
        "- Supervised Learning is learning in which known labels are assigned to indivudual examples, and the primary goal of the machine is to predict labels for future, unknown examples. In general, this allows for what I would consider more useful machines, such as ones that can identify the locations of humans in a picture, ones that can more easily determine whether a dog, cat, or hamster is shown in a picture, or ones that determine the position of the road to drive on when given a picture from a car's camera.\n",
        "- Unsupervised learning is helpful in times in which we have a boatload of data but we don't have the time or resources to classify it by hand in order to use it as input to some network. Unsupervised learning can be useful when we, as humans, don't really know what it is we are looking for, and we would just like the computer to identify some sort of general classifications that it might find in the data. \n",
        "- This might be helpful if, for example, we are attempting to analyze and classify the performance of high-level players in a video game we are not ourselves very good at. If we can identify certain common traits or playstyles among groups of talented players (whether the players are human or computers isn't relevant here), we might be able to make predictions in the future about how a particular player might act in some situation based on which group he is in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdv6_dymVZ5M",
        "colab_type": "text"
      },
      "source": [
        "#Basic Machine Learning Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvKB-F6Pd5pG",
        "colab_type": "text"
      },
      "source": [
        "The fundemental example of how to use machine learning is finding a linear pattern based on a series of inputs to predict some output. I claim that pretty much all other machine learning after this is an extrapolation of this idea and a clever use of activation functions and convolution.\n",
        "\n",
        "**Linear Regression** - We are given a vector of inputs of some sort and we want to use these inputs to predict some output that we believe is some linear combination of the inputs. Suppose, for instance, that we are given two input measurements `height` and `weight`, and we want to predict `belt_size`. This seems like, for the most part, it should be rather simple to use these two inputs to predict the output, so it makes sense to try using a linear model. \n",
        "\n",
        "Our approach will be to try to identify two real numbers `a` and `b` so that we can accurately predict `belt_size` to be approximately equal to `a*height+b*weight`. If we can find a good `a` and `b` then we can use those to predict what the belt size will be for customers for whom we don't already know the belt size if we know their height and weight.\n",
        "\n",
        "**Logistic Regression** - In logistic regression, we are again give a series of inputs which we want to use to predict some output. However, in logistic regression, we want to use the inputs to predict some class of output. Since this assignment tends to have a series of continued examples, one example use case of this might be a model that is given a person'e height and weight and then needs to determine whether that person is a male or a female (possibly in order to direct them to the correct sizing chart). The input to this model would again be a vector of real numbers, and the output would be a signle real number between 0 and 1 that represents our estimated probability that this individual is male. To enforce that our output is between 0 and 1 (i. e. that it is a probability), we can use the sigmoid activation function which will convert any positive real number to a probability > 0.5, and any negative real number to a probability < 0.5. \n",
        "\n",
        "**Gradients and Loss** - In order to accurately quantify exactly how well our model is prediciting our test data, we can define a loss function. This function essentially encapsulates a lack of confidence about the correct solution combined with false confidence in an incorrect solution. So, for instance, consider the following examples:\n",
        "\n",
        "```\n",
        "weight (kg)  height (m)     predicted chance male   actual gender\n",
        "    1          11                     0.05              Female\n",
        "    1.1        1_000                  0.61              Female\n",
        "```\n",
        "We want to create a loss function so that the first example has relatively low loss (we correctly predicted the gender with much confidence), the the section example has a much higher loss (as we incorrectly predicted the gender). Binary Crossentropy loss would be helpful in this case. If, instead we were looking at an example in which we needed loss for linear regression, the square of the difference between the predicted y, called y-hat, and the actual y would serve as a quick and easy loss function.\n",
        "\n",
        "Since a good hueristic of our goal is to minimize loss (technically it is usually to maximize accuracy, but if we can somehow minimize our loss that would likely result in a high accuracy), we can adjust our weights in such a way that it will lower our loss for a given example. We can do this repeatedly for many examples to gradually improve our weights until they become useful for predicting examples we haven't seen.\n",
        "\n",
        "**Gradient Descent** - But how do we do this? We can calculate the derivative of loss with respect to each of our weights. That is, for some weight, if we increased that weight by a small amount, how would that affect our loss? It should be intuitive to see that if increasing that weight would also increase our loss, this derivative would then be positive, and this would be a bad idea (we should therefore likely decrease this weight). In other words, if we repeatedly calculate the gradient for each example, we can adjust our weights against this gradient and eventually descend on the location in which our loss is minimal, hopefully maximizing accuracy. This works quite well in convex problems, and also in simpler problems in which the goal is simply to identify an equation that was used to generate this data in the first place.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FvbK536VaDE",
        "colab_type": "text"
      },
      "source": [
        "#Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2tHHxqgOOS",
        "colab_type": "text"
      },
      "source": [
        "**Convolutional Neural Networks** - In instances in which we need to do some sort of image or sound processing, it is usually quite helpful to use Convolutional Neural Networks (known as CNNs, unrelated to the media company). The reason for this is that it allows us to process a large amount of our image with comparatively few weights. The idea behind a CNN is that, instead of looking at the raw brightness of each pixel and comparing that to every output pixel, we can identify some *kernal* that we can *convolve* accross the image. In this context, a kernal is nothing more than a 2D matrix of weights. Convolving a kernal simply means placing that kernal at each possible location in the image, and calculating the product of pairwise-overlapping cells, and then taking their sum (in the same way that the dot product is calculated). \n",
        "\n",
        "Why is this helpful? Well techincally it isn't strictly necessary. If you really wanted to, you could theoretically construct another graph that had all of the same edges as this convolutional layer, and that would wind up giving you the same result (you would have a lot of edges that were just 0s, but it would work). The main issue with doing it this way is that you would have a very very large number of weights that you would have to balance out, and it would be quite difficult to do that effectively. For instance, if you had a 150x150 input layer, and you had a 50x50 kernal, there would be roughly 150x150x100x100 weights in a naively implemented DNN. However, using a CNN, we can use just 50x50 weights, and then use those weights multiple times. This allows us to find a useful set of weights much more quickly, since it significantly reduces our search space.\n",
        "\n",
        "###Components of a Nueral Network\n",
        "A nueral network is composed of a series of layers. Each one of these layers has a series of weights and identifies new patterns in the data. There are several different types of layers that are each useful to know and understand. Some important ones are the following:\n",
        "- Dense layers- Fully connected to the previous layer. If the previous layer has n nodes and this layer has m nodes, this will consist of n*m unique edges.\n",
        "- Activation layers- Although activation functions can be (and almost always are) embedded in dense layers for you, you can add your own activation layers if you need to do something like use a sigmoid activation function like I mentioned earlier. \n",
        "- Dropout layers- Randomly sets some of the inputs to 0 at each frame in order to prevent overdependence on a small subset of nodes. This tends to happen if a model is overfitting its test data, so dropout layers can be used to combat that.\n",
        "- Flatten layers- These don't actually affect the data going through the network, it just turns a multidimensional array of nodes into a single vector of nodes (often so that it can be connected to a following dense layer, for instance). Since this doesn't really change the input data at all, these layers don't contribute any extra edge weights that need to be adjusted in training.\n",
        "- Recurrent nueral networks of various types: There are many different types of recurrant neural networks that are supported by Keras and other deep learning libraries, but a) we didn't spend much time talking about them in this class, and b) they aren't all that necessary for creating an effective network, so I won't cover them in too much detail here.\n",
        "- Convolutional layers- These layers are used for CNNs as described above. They provide a kernal that is convolved through the image and creates another layer that is the output of that convolution.\n",
        "\n",
        "###Structure of CNNs\n",
        "In this class we primarily covered sequential models in which the different layers occur one after another in sequence, as the name would suggest. Other models exist in which certain convolutions or pooling operations are done in parallel of each other, which allows for different behaviour.\n",
        "\n",
        "In image processing networks, earlier layers might do nothing more than detect vertial and horizontal edges or ripples in an image, while later layers might detect things like eyes or ear shapes. \n",
        "\n",
        "Formally, CNNs usually consist of two main parts: a convultional base, and a classifier. The convolutional base identifies important traits in the image, while the classifier looks at these traits, and determines what the image is likely to be. This structure is convenient for two reasons: the first is that it allows for the convolutional layers to be seperated from the dense layers, which makes logical sense. The second is that it allows for useful pretrained networks to be reused for different classifications, by not including a custom classifier instead of whatever the one in the previous version was."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB7FjHYkVaJ0",
        "colab_type": "text"
      },
      "source": [
        "#Optimizing a Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx0dEN4NgkQ_",
        "colab_type": "text"
      },
      "source": [
        "**Optimizers** - The optimizer for a model is the method of improving the weights of that model. The simplest effective optimizer is Stochastic Gradient Descent (sgd). Stochastic gradient descent adjusts the weights of a model using gradient descent in the same way as gradient descent mentioned above, and will adjust gradients every time a single example is processed. This is easier to code, but it obviously results in way more weight updates, which makes it difficult to parallelize. \n",
        "\n",
        "Batch gradient descent is the polar opposite of SGD, and as the name suggests, involves processing an entire epoch as a batch. Once all of the test data has been processed, the weights are updated once. Using BGD, the updates to weights are usually more accurate and less sporatic, so often a larger learning rate can be used safely.\n",
        "\n",
        "A good compromise between BGD and SGD is Semi-batch gradient descent. SBGD uses a smaller batch size than an entire epoch, and therefore it is what is usually used in training models. Usually a batch size of ~4 or ~8 is very effective since it allows your computer to effectively use multiple cores or multiple threads behind the scenes to parallelize computation in between weight updates, but it also gives rather frequent weight updates, which allows for good solutions to be quickly identified.\n",
        "\n",
        "**Learning Rate** - The learning rate given to a model is the amount that the edge weights are updated during a single weight update. A larger learning rate will result in more quickly getting an appropriate solution (in theory) but also leads to less fidelity. Too slow of a learning rate will result in a model taking an excessively long time to train, which is mostly just annoying and inconvenient.\n",
        "\n",
        "However, having too large of a learning rate can be a serious issue which can make it impossible to train a model that is actually useful. The potential issue that needs to be avoided is consistantly overshooting the local minimum. Overshooting the minimum slightly is okay, as on the next iteration you will move back to the appropriate location. However, if you overshoot the minimum in such a way that your loss is now larger, but you are on the other side of the minimum, your model can springboard between sides of the minimum and actually get farther and farther away from the desired weights, causing further training to be unproductive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T8Dqk4MVaQ4",
        "colab_type": "text"
      },
      "source": [
        "#Training a  Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxLjqpxpgo2U",
        "colab_type": "text"
      },
      "source": [
        "Training a model properly involves quite a bit of guesswork. It is usually very helpful to plot the model's statistics on accuracy and loss on both the test data and on the validation data. If your learning rate is small enough (as discussed above), and it appears that your model still fails to increase accuracy on test data, it is possible that your model is suffering from underfitting.\n",
        "\n",
        "**Underfitting** - Underfitting occurs when the topology of a network cannot adaquately represent the complexity of whatever it is that the model is trying to predict. Say, for instance, you are trying to determine whether a picture represents a cat or a dog with only a single dense layer in the entire network. Processing this image is clearly a complicated task; it isn't something that can be easily condensed down to just the a linear combination of pixel brightnesses. In this case, no matter how many epochs you train your model on any reasonable amount of test data, it is very unlikely that your model will have test accuracy that is as good as you would like. \n",
        "\n",
        "In this case, your model would be underfitting the true data; you model can only represent a n-dimensional plane, since it isn't using any helpful intermediate activation functions. The set of points that represent pictures of dogs and pictures of cats is very likely a complex shape that cannot be easily seperated by a plane. To handle this, a more complex model is required.\n",
        "\n",
        "**Overfitting** - On the other hand, it is possible to have a model that is *too* complex for a given problem. It is difficult to describe an exact case in which this would always happen, because there are many techniques that can be used to appropraite address and combat overfitting. One symptom of this happening is that the test accuracy of the model continues to increase and approach 100%, while the validation accuracy tops off at some lower number, say 93%. In this case, after the validation accuracy stops increasing, you know that your model isn't actually tuning its weights in a helpful way anymore. Instead, now, all that it is doing is identifying unhelpful patterns in the training data that don't actually exist in the complete set of pictures of, say, dogs and cats. You can tell that this is the case because those observations that your model is making about the training data set do not apply to the validation data set, which corresponds to the discrepancy in accuracy.\n",
        "\n",
        "As mentioned above, one helpful tool to prevent overfitting is using dropout. This helps avoid unnecessary dependence on just a few nodes in the network, which likely come from small patterns that happen to be present in the training data, just by chance. Another possible tool (which is much more difficult to actually use in practice) is to not make your model unnecessarily complex. If you are modeling an approximately linear system, a linear model will not overfit that system as long as you have adaquate training data. The only way you will get overfitting is if you include too many layers or too complicated activation functions that allow your model to seek out exceptions for minor anomylies in your training data that in reality it should just be getting wrong.\n",
        "\n",
        "Finally, one last trick that can be used to prevent overfitting is to use some sort of data augmentation, especailly for CNNs that are processing images. If your data is of an image, modifying that image in some minor way will likely result in a picture that, to a human observer, should likely be pretty much of the same thing. These modifications can include cropping the image to take some random rectangle in it (that doesn't remove too much info from the image), or mildly adjusting the image's exposure or saturation, or to superimposing some other image with very light alpha. In each of these cases, a model that can still predict what the original image was despite these modifications will likely be more robust because it will be able to identify useful patterns that actually describe the differences between, say, dogs and cats, instead of identifying trivial in the data set, such as: the 342nd pixel on the 71st row is almost alwas darker for pictures of dogs than it is for pictures of cats in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNngR3NUVaWY",
        "colab_type": "text"
      },
      "source": [
        "#Finetuning a Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd17dZmKL8x_",
        "colab_type": "text"
      },
      "source": [
        "Although it is fun and fills students with a sense of pride to train their own nueral network from scratch, in the real world we can usually accomplish more with fewer resources by taking advantage of known existing nueral networks.\n",
        "\n",
        "Publicly available pretrained nueral networks such as ResNet can be found online and freely downloaded. These networks have already been trained to identify quite meaningful features in images, and thar training can be reused for other purposes. As I mentioned in the CNN section of this assignment, most of this knowledge is distilled in the convolutional base of the network. We can remove the classifier from these networks and add our own custom classifier, but keep the old convolutional base in order have a very useful jump-start on our model.\n",
        "\n",
        "One quite important detail that is important to remember is that, when doing this, we initially start with a classifier that is initially full of just random garbage. If we train our model on this, we will destroy the useful information in the convolutional base right as we begin training the network, and it will be so bad that we will pretty much have lost all usefulness that we gained from having the pretrained network in the first place.\n",
        "\n",
        "In order to combat this, we can prevent ourselves from destroying the conv_base by making it not trainable for the first ~30 epochs. Then, once we have a quite good classifier, we can make some fine adjustments to the conv_base by allowing the last few layers of it to be tuned. By doing this, we can identify important features that it might have been missing earlier, but we can find those without trampling over the important features that it was finding."
      ]
    }
  ]
}